{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7cb7f33-5fe3-4fc7-98fd-2c818d634c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-28 15:22:18.978258: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-28 15:22:19.053459: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-28 15:22:19.073615: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-28 15:22:19.396758: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64:\n",
      "2022-11-28 15:22:19.396805: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64:\n",
      "2022-11-28 15:22:19.396809: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dlt\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPool2D\n",
    "from keras.layers.core import Dense,Activation,Dropout,Flatten\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cf1a0ee-23e8-4873-b701-ec4709aa7521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-28 15:22:36.796939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-28 15:22:36.815202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-28 15:22:36.815297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-28 15:22:36.815596: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-28 15:22:36.816355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-28 15:22:36.816452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-28 15:22:36.816527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-28 15:22:37.089103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-28 15:22:37.089231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-28 15:22:37.089311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-28 15:22:37.089377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14236 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-28 15:22:38.101586: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600\n",
      "2022-11-28 15:22:38.551976: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 2s 19ms/step - loss: 2.1172 - accuracy: 0.2100 - val_loss: 1.9601 - val_accuracy: 0.2720\n",
      "Epoch 2/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.7905 - accuracy: 0.3407 - val_loss: 1.6383 - val_accuracy: 0.4300\n",
      "Epoch 3/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.5852 - accuracy: 0.4244 - val_loss: 1.5689 - val_accuracy: 0.4380\n",
      "Epoch 4/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.4745 - accuracy: 0.4616 - val_loss: 1.4563 - val_accuracy: 0.4860\n",
      "Epoch 5/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.3536 - accuracy: 0.5067 - val_loss: 1.3678 - val_accuracy: 0.4720\n",
      "Epoch 6/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.2230 - accuracy: 0.5633 - val_loss: 1.2856 - val_accuracy: 0.5380\n",
      "Epoch 7/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.1176 - accuracy: 0.6007 - val_loss: 1.2294 - val_accuracy: 0.5680\n",
      "Epoch 8/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.9667 - accuracy: 0.6580 - val_loss: 1.3134 - val_accuracy: 0.5260\n",
      "Epoch 9/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.8408 - accuracy: 0.7011 - val_loss: 1.2679 - val_accuracy: 0.5760\n",
      "Epoch 10/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.6678 - accuracy: 0.7671 - val_loss: 1.6416 - val_accuracy: 0.5300\n",
      "Epoch 11/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.5813 - accuracy: 0.7982 - val_loss: 1.3235 - val_accuracy: 0.5920\n",
      "Epoch 12/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.4013 - accuracy: 0.8609 - val_loss: 1.3941 - val_accuracy: 0.6120\n",
      "Epoch 13/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.3081 - accuracy: 0.8958 - val_loss: 1.4541 - val_accuracy: 0.6080\n",
      "Epoch 14/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.2321 - accuracy: 0.9229 - val_loss: 1.5896 - val_accuracy: 0.5820\n",
      "Epoch 15/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1756 - accuracy: 0.9409 - val_loss: 1.6078 - val_accuracy: 0.6060\n",
      "Epoch 16/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1302 - accuracy: 0.9547 - val_loss: 2.0228 - val_accuracy: 0.5800\n",
      "Epoch 17/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1465 - accuracy: 0.9509 - val_loss: 1.8467 - val_accuracy: 0.5700\n",
      "Epoch 18/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1255 - accuracy: 0.9582 - val_loss: 1.8986 - val_accuracy: 0.5900\n",
      "Epoch 19/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.0910 - accuracy: 0.9702 - val_loss: 2.0539 - val_accuracy: 0.5680\n",
      "Epoch 20/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.0859 - accuracy: 0.9764 - val_loss: 2.0830 - val_accuracy: 0.5820\n",
      "Test loss: 2.4862639904022217\n",
      "Test accuracy: 0.5730000138282776\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "0.64185\n",
      "Epoch 1/20\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 2.1561 - accuracy: 0.1938 - val_loss: 2.1645 - val_accuracy: 0.2040\n",
      "Epoch 2/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.8467 - accuracy: 0.3244 - val_loss: 1.7299 - val_accuracy: 0.3580\n",
      "Epoch 3/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.6635 - accuracy: 0.3838 - val_loss: 1.5751 - val_accuracy: 0.4500\n",
      "Epoch 4/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.5492 - accuracy: 0.4378 - val_loss: 1.5344 - val_accuracy: 0.4420\n",
      "Epoch 5/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.4405 - accuracy: 0.4782 - val_loss: 1.5002 - val_accuracy: 0.4600\n",
      "Epoch 6/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.3385 - accuracy: 0.5138 - val_loss: 1.4041 - val_accuracy: 0.4960\n",
      "Epoch 7/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.1922 - accuracy: 0.5709 - val_loss: 1.5645 - val_accuracy: 0.4480\n",
      "Epoch 8/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.0939 - accuracy: 0.6087 - val_loss: 1.4403 - val_accuracy: 0.5000\n",
      "Epoch 9/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.9526 - accuracy: 0.6658 - val_loss: 1.3819 - val_accuracy: 0.5180\n",
      "Epoch 10/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.8213 - accuracy: 0.7049 - val_loss: 1.4354 - val_accuracy: 0.5100\n",
      "Epoch 11/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.6650 - accuracy: 0.7669 - val_loss: 1.3755 - val_accuracy: 0.5460\n",
      "Epoch 12/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.5303 - accuracy: 0.8129 - val_loss: 1.5185 - val_accuracy: 0.5480\n",
      "Epoch 13/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.4263 - accuracy: 0.8551 - val_loss: 1.5595 - val_accuracy: 0.5500\n",
      "Epoch 14/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.3340 - accuracy: 0.8838 - val_loss: 1.5804 - val_accuracy: 0.5380\n",
      "Epoch 15/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.2875 - accuracy: 0.9007 - val_loss: 1.6666 - val_accuracy: 0.5380\n",
      "Epoch 16/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1746 - accuracy: 0.9402 - val_loss: 2.0516 - val_accuracy: 0.5540\n",
      "Epoch 17/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1437 - accuracy: 0.9533 - val_loss: 1.9591 - val_accuracy: 0.5520\n",
      "Epoch 18/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1257 - accuracy: 0.9580 - val_loss: 2.1276 - val_accuracy: 0.5660\n",
      "Epoch 19/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1529 - accuracy: 0.9502 - val_loss: 2.0951 - val_accuracy: 0.5780\n",
      "Epoch 20/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1566 - accuracy: 0.9496 - val_loss: 2.0323 - val_accuracy: 0.5660\n",
      "Test loss: 2.262810230255127\n",
      "Test accuracy: 0.5490000247955322\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "0.55705\n",
      "Epoch 1/20\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 2.1779 - accuracy: 0.1862 - val_loss: 1.9063 - val_accuracy: 0.3380\n",
      "Epoch 2/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.7827 - accuracy: 0.3433 - val_loss: 1.7261 - val_accuracy: 0.3880\n",
      "Epoch 3/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.6161 - accuracy: 0.4133 - val_loss: 1.5627 - val_accuracy: 0.4460\n",
      "Epoch 4/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.4999 - accuracy: 0.4569 - val_loss: 1.5236 - val_accuracy: 0.4700\n",
      "Epoch 5/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.4035 - accuracy: 0.4873 - val_loss: 1.3619 - val_accuracy: 0.5280\n",
      "Epoch 6/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.2495 - accuracy: 0.5431 - val_loss: 1.3155 - val_accuracy: 0.5300\n",
      "Epoch 7/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.1397 - accuracy: 0.5920 - val_loss: 1.3614 - val_accuracy: 0.5380\n",
      "Epoch 8/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.9931 - accuracy: 0.6384 - val_loss: 1.3950 - val_accuracy: 0.5220\n",
      "Epoch 9/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.8613 - accuracy: 0.6991 - val_loss: 1.3908 - val_accuracy: 0.5400\n",
      "Epoch 10/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.6985 - accuracy: 0.7520 - val_loss: 1.3766 - val_accuracy: 0.5520\n",
      "Epoch 11/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.5646 - accuracy: 0.8120 - val_loss: 1.4240 - val_accuracy: 0.5500\n",
      "Epoch 12/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.4391 - accuracy: 0.8429 - val_loss: 1.6038 - val_accuracy: 0.5440\n",
      "Epoch 13/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.3181 - accuracy: 0.8833 - val_loss: 1.7430 - val_accuracy: 0.5320\n",
      "Epoch 14/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.2811 - accuracy: 0.9049 - val_loss: 1.7397 - val_accuracy: 0.5360\n",
      "Epoch 15/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1839 - accuracy: 0.9387 - val_loss: 1.9935 - val_accuracy: 0.5700\n",
      "Epoch 16/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1842 - accuracy: 0.9424 - val_loss: 2.1199 - val_accuracy: 0.5500\n",
      "Epoch 17/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1447 - accuracy: 0.9522 - val_loss: 2.0670 - val_accuracy: 0.5560\n",
      "Epoch 18/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1082 - accuracy: 0.9667 - val_loss: 2.0584 - val_accuracy: 0.5660\n",
      "Epoch 19/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.0864 - accuracy: 0.9724 - val_loss: 2.3979 - val_accuracy: 0.5560\n",
      "Epoch 20/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.0817 - accuracy: 0.9727 - val_loss: 2.3880 - val_accuracy: 0.5260\n",
      "Test loss: 2.5552895069122314\n",
      "Test accuracy: 0.5569999814033508\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "0.6282\n",
      "Epoch 1/20\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 2.0964 - accuracy: 0.2153 - val_loss: 1.8123 - val_accuracy: 0.3260\n",
      "Epoch 2/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.7410 - accuracy: 0.3502 - val_loss: 1.8562 - val_accuracy: 0.3520\n",
      "Epoch 3/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.6037 - accuracy: 0.4198 - val_loss: 1.5015 - val_accuracy: 0.4840\n",
      "Epoch 4/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.4583 - accuracy: 0.4682 - val_loss: 1.4870 - val_accuracy: 0.4680\n",
      "Epoch 5/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.3191 - accuracy: 0.5209 - val_loss: 1.3450 - val_accuracy: 0.5100\n",
      "Epoch 6/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.1883 - accuracy: 0.5702 - val_loss: 1.2394 - val_accuracy: 0.5520\n",
      "Epoch 7/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.0719 - accuracy: 0.6122 - val_loss: 1.2148 - val_accuracy: 0.5440\n",
      "Epoch 8/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.9720 - accuracy: 0.6536 - val_loss: 1.2506 - val_accuracy: 0.5840\n",
      "Epoch 9/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.8416 - accuracy: 0.7044 - val_loss: 1.3462 - val_accuracy: 0.5560\n",
      "Epoch 10/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.7173 - accuracy: 0.7538 - val_loss: 1.1934 - val_accuracy: 0.6200\n",
      "Epoch 11/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.5273 - accuracy: 0.8140 - val_loss: 1.4934 - val_accuracy: 0.5660\n",
      "Epoch 12/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.4657 - accuracy: 0.8391 - val_loss: 1.3377 - val_accuracy: 0.5980\n",
      "Epoch 13/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.3498 - accuracy: 0.8802 - val_loss: 1.5627 - val_accuracy: 0.5880\n",
      "Epoch 14/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.2628 - accuracy: 0.9089 - val_loss: 1.6964 - val_accuracy: 0.5920\n",
      "Epoch 15/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1988 - accuracy: 0.9349 - val_loss: 1.7017 - val_accuracy: 0.5940\n",
      "Epoch 16/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1669 - accuracy: 0.9444 - val_loss: 1.8478 - val_accuracy: 0.5980\n",
      "Epoch 17/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1482 - accuracy: 0.9489 - val_loss: 1.8959 - val_accuracy: 0.5800\n",
      "Epoch 18/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1433 - accuracy: 0.9522 - val_loss: 1.7606 - val_accuracy: 0.6000\n",
      "Epoch 19/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.0873 - accuracy: 0.9742 - val_loss: 1.9632 - val_accuracy: 0.6200\n",
      "Epoch 20/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.0673 - accuracy: 0.9784 - val_loss: 2.2165 - val_accuracy: 0.5980\n",
      "Test loss: 2.435699701309204\n",
      "Test accuracy: 0.578000009059906\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "0.59455\n",
      "Epoch 1/20\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 2.0791 - accuracy: 0.2162 - val_loss: 1.9236 - val_accuracy: 0.3200\n",
      "Epoch 2/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.7708 - accuracy: 0.3467 - val_loss: 1.6811 - val_accuracy: 0.3940\n",
      "Epoch 3/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.5856 - accuracy: 0.4180 - val_loss: 1.6241 - val_accuracy: 0.4380\n",
      "Epoch 4/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.4687 - accuracy: 0.4611 - val_loss: 1.5055 - val_accuracy: 0.4580\n",
      "Epoch 5/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.3504 - accuracy: 0.5129 - val_loss: 1.4009 - val_accuracy: 0.5080\n",
      "Epoch 6/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.2138 - accuracy: 0.5649 - val_loss: 1.3701 - val_accuracy: 0.4940\n",
      "Epoch 7/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.0916 - accuracy: 0.6071 - val_loss: 1.4476 - val_accuracy: 0.5120\n",
      "Epoch 8/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.9810 - accuracy: 0.6469 - val_loss: 1.4295 - val_accuracy: 0.4780\n",
      "Epoch 9/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.8304 - accuracy: 0.7038 - val_loss: 1.3211 - val_accuracy: 0.5320\n",
      "Epoch 10/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.6965 - accuracy: 0.7500 - val_loss: 1.3692 - val_accuracy: 0.5500\n",
      "Epoch 11/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.5391 - accuracy: 0.8116 - val_loss: 1.4172 - val_accuracy: 0.5680\n",
      "Epoch 12/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.4358 - accuracy: 0.8447 - val_loss: 1.5159 - val_accuracy: 0.5480\n",
      "Epoch 13/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.3552 - accuracy: 0.8749 - val_loss: 1.5242 - val_accuracy: 0.5780\n",
      "Epoch 14/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.2358 - accuracy: 0.9236 - val_loss: 1.6927 - val_accuracy: 0.5680\n",
      "Epoch 15/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.2143 - accuracy: 0.9256 - val_loss: 1.8489 - val_accuracy: 0.5480\n",
      "Epoch 16/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1896 - accuracy: 0.9380 - val_loss: 1.8376 - val_accuracy: 0.5860\n",
      "Epoch 17/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1142 - accuracy: 0.9622 - val_loss: 1.9396 - val_accuracy: 0.5620\n",
      "Epoch 18/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.0902 - accuracy: 0.9709 - val_loss: 2.1415 - val_accuracy: 0.5720\n",
      "Epoch 19/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.0974 - accuracy: 0.9660 - val_loss: 2.0292 - val_accuracy: 0.5640\n",
      "Epoch 20/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.0698 - accuracy: 0.9798 - val_loss: 2.2771 - val_accuracy: 0.5440\n",
      "Test loss: 2.5246760845184326\n",
      "Test accuracy: 0.5799999833106995\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "0.6296\n",
      "Epoch 1/20\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 2.1064 - accuracy: 0.2131 - val_loss: 1.9097 - val_accuracy: 0.2760\n",
      "Epoch 2/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.7787 - accuracy: 0.3520 - val_loss: 1.7493 - val_accuracy: 0.4060\n",
      "Epoch 3/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.5905 - accuracy: 0.4171 - val_loss: 1.5865 - val_accuracy: 0.4220\n",
      "Epoch 4/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.4383 - accuracy: 0.4724 - val_loss: 1.4219 - val_accuracy: 0.4800\n",
      "Epoch 5/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.3735 - accuracy: 0.5007 - val_loss: 1.4265 - val_accuracy: 0.4860\n",
      "Epoch 6/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.2131 - accuracy: 0.5587 - val_loss: 1.4399 - val_accuracy: 0.4820\n",
      "Epoch 7/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.0880 - accuracy: 0.6033 - val_loss: 1.2440 - val_accuracy: 0.5560\n",
      "Epoch 8/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.9869 - accuracy: 0.6424 - val_loss: 1.3923 - val_accuracy: 0.5240\n",
      "Epoch 9/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.8460 - accuracy: 0.7078 - val_loss: 1.2864 - val_accuracy: 0.5620\n",
      "Epoch 10/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.7086 - accuracy: 0.7520 - val_loss: 1.3441 - val_accuracy: 0.5660\n",
      "Epoch 11/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.5381 - accuracy: 0.8109 - val_loss: 1.4950 - val_accuracy: 0.5520\n",
      "Epoch 12/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.4411 - accuracy: 0.8469 - val_loss: 1.4572 - val_accuracy: 0.5340\n",
      "Epoch 13/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.3520 - accuracy: 0.8816 - val_loss: 1.6471 - val_accuracy: 0.5660\n",
      "Epoch 14/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.2363 - accuracy: 0.9198 - val_loss: 1.7830 - val_accuracy: 0.5660\n",
      "Epoch 15/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.2047 - accuracy: 0.9307 - val_loss: 1.8422 - val_accuracy: 0.5600\n",
      "Epoch 16/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1411 - accuracy: 0.9518 - val_loss: 1.9248 - val_accuracy: 0.5780\n",
      "Epoch 17/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1197 - accuracy: 0.9571 - val_loss: 2.0263 - val_accuracy: 0.5460\n",
      "Epoch 18/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1248 - accuracy: 0.9600 - val_loss: 1.8910 - val_accuracy: 0.5760\n",
      "Epoch 19/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.0925 - accuracy: 0.9693 - val_loss: 2.2469 - val_accuracy: 0.5920\n",
      "Epoch 20/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.0701 - accuracy: 0.9769 - val_loss: 2.3204 - val_accuracy: 0.5540\n",
      "Test loss: 2.510328769683838\n",
      "Test accuracy: 0.5600000023841858\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "0.6743\n",
      "Epoch 1/20\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 2.1479 - accuracy: 0.2000 - val_loss: 1.8994 - val_accuracy: 0.3080\n",
      "Epoch 2/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.7875 - accuracy: 0.3440 - val_loss: 1.7103 - val_accuracy: 0.3680\n",
      "Epoch 3/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.6130 - accuracy: 0.4176 - val_loss: 1.4783 - val_accuracy: 0.4860\n",
      "Epoch 4/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.4254 - accuracy: 0.4782 - val_loss: 1.4497 - val_accuracy: 0.4780\n",
      "Epoch 5/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.3169 - accuracy: 0.5173 - val_loss: 1.3425 - val_accuracy: 0.5300\n",
      "Epoch 6/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.1771 - accuracy: 0.5671 - val_loss: 1.2730 - val_accuracy: 0.5200\n",
      "Epoch 7/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.0401 - accuracy: 0.6269 - val_loss: 1.3270 - val_accuracy: 0.5220\n",
      "Epoch 8/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.9032 - accuracy: 0.6736 - val_loss: 1.2176 - val_accuracy: 0.5800\n",
      "Epoch 9/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.7517 - accuracy: 0.7378 - val_loss: 1.3068 - val_accuracy: 0.5660\n",
      "Epoch 10/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.6371 - accuracy: 0.7787 - val_loss: 1.3476 - val_accuracy: 0.5660\n",
      "Epoch 11/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.4632 - accuracy: 0.8360 - val_loss: 1.4773 - val_accuracy: 0.5320\n",
      "Epoch 12/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.4101 - accuracy: 0.8584 - val_loss: 1.5244 - val_accuracy: 0.5960\n",
      "Epoch 13/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.2785 - accuracy: 0.9076 - val_loss: 1.6053 - val_accuracy: 0.5760\n",
      "Epoch 14/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.2329 - accuracy: 0.9227 - val_loss: 1.5612 - val_accuracy: 0.5980\n",
      "Epoch 15/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1980 - accuracy: 0.9320 - val_loss: 1.7206 - val_accuracy: 0.5900\n",
      "Epoch 16/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1632 - accuracy: 0.9438 - val_loss: 1.8254 - val_accuracy: 0.5900\n",
      "Epoch 17/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1064 - accuracy: 0.9638 - val_loss: 1.8918 - val_accuracy: 0.5960\n",
      "Epoch 18/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.0763 - accuracy: 0.9760 - val_loss: 1.9881 - val_accuracy: 0.5880\n",
      "Epoch 19/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.0772 - accuracy: 0.9758 - val_loss: 2.1092 - val_accuracy: 0.5840\n",
      "Epoch 20/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.0839 - accuracy: 0.9756 - val_loss: 2.0786 - val_accuracy: 0.5820\n",
      "Test loss: 2.3367221355438232\n",
      "Test accuracy: 0.5479999780654907\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "0.60915\n",
      "Epoch 1/20\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 2.1153 - accuracy: 0.2053 - val_loss: 1.9454 - val_accuracy: 0.3040\n",
      "Epoch 2/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.7775 - accuracy: 0.3549 - val_loss: 1.6699 - val_accuracy: 0.3820\n",
      "Epoch 3/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.5672 - accuracy: 0.4284 - val_loss: 1.5147 - val_accuracy: 0.4400\n",
      "Epoch 4/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.4307 - accuracy: 0.4711 - val_loss: 1.3913 - val_accuracy: 0.4800\n",
      "Epoch 5/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.3180 - accuracy: 0.5229 - val_loss: 1.3641 - val_accuracy: 0.4920\n",
      "Epoch 6/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.2036 - accuracy: 0.5636 - val_loss: 1.2702 - val_accuracy: 0.5360\n",
      "Epoch 7/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.0305 - accuracy: 0.6309 - val_loss: 1.2538 - val_accuracy: 0.5340\n",
      "Epoch 8/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.8609 - accuracy: 0.6951 - val_loss: 1.2932 - val_accuracy: 0.5520\n",
      "Epoch 9/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.7528 - accuracy: 0.7369 - val_loss: 1.2882 - val_accuracy: 0.5380\n",
      "Epoch 10/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.5624 - accuracy: 0.8033 - val_loss: 1.3753 - val_accuracy: 0.5840\n",
      "Epoch 11/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.4782 - accuracy: 0.8287 - val_loss: 1.4059 - val_accuracy: 0.5560\n",
      "Epoch 12/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.3466 - accuracy: 0.8827 - val_loss: 1.4351 - val_accuracy: 0.5700\n",
      "Epoch 13/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.2561 - accuracy: 0.9120 - val_loss: 1.6590 - val_accuracy: 0.5760\n",
      "Epoch 14/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1858 - accuracy: 0.9371 - val_loss: 1.7594 - val_accuracy: 0.5680\n",
      "Epoch 15/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1409 - accuracy: 0.9524 - val_loss: 1.8241 - val_accuracy: 0.5760\n",
      "Epoch 16/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1123 - accuracy: 0.9667 - val_loss: 1.8922 - val_accuracy: 0.5780\n",
      "Epoch 17/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1218 - accuracy: 0.9611 - val_loss: 2.0441 - val_accuracy: 0.5720\n",
      "Epoch 18/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1057 - accuracy: 0.9678 - val_loss: 1.9138 - val_accuracy: 0.5840\n",
      "Epoch 19/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.0745 - accuracy: 0.9773 - val_loss: 2.0665 - val_accuracy: 0.5820\n",
      "Epoch 20/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.0570 - accuracy: 0.9822 - val_loss: 2.2566 - val_accuracy: 0.5880\n",
      "Test loss: 2.532870054244995\n",
      "Test accuracy: 0.574999988079071\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "0.6278\n",
      "Epoch 1/20\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 2.1721 - accuracy: 0.1938 - val_loss: 1.9516 - val_accuracy: 0.2640\n",
      "Epoch 2/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.7925 - accuracy: 0.3540 - val_loss: 1.6762 - val_accuracy: 0.3920\n",
      "Epoch 3/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.6451 - accuracy: 0.3907 - val_loss: 1.5597 - val_accuracy: 0.4320\n",
      "Epoch 4/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.4901 - accuracy: 0.4529 - val_loss: 1.4854 - val_accuracy: 0.4720\n",
      "Epoch 5/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.3613 - accuracy: 0.5138 - val_loss: 1.3882 - val_accuracy: 0.5120\n",
      "Epoch 6/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.2447 - accuracy: 0.5556 - val_loss: 1.3101 - val_accuracy: 0.5240\n",
      "Epoch 7/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.1164 - accuracy: 0.5927 - val_loss: 1.2648 - val_accuracy: 0.5460\n",
      "Epoch 8/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.0122 - accuracy: 0.6380 - val_loss: 1.3206 - val_accuracy: 0.4940\n",
      "Epoch 9/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.8638 - accuracy: 0.7029 - val_loss: 1.2074 - val_accuracy: 0.5420\n",
      "Epoch 10/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.7125 - accuracy: 0.7462 - val_loss: 1.3248 - val_accuracy: 0.5640\n",
      "Epoch 11/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.6091 - accuracy: 0.7867 - val_loss: 1.4547 - val_accuracy: 0.5480\n",
      "Epoch 12/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.5389 - accuracy: 0.8118 - val_loss: 1.3953 - val_accuracy: 0.6020\n",
      "Epoch 13/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.3817 - accuracy: 0.8691 - val_loss: 1.4626 - val_accuracy: 0.5760\n",
      "Epoch 14/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.2936 - accuracy: 0.9011 - val_loss: 1.5716 - val_accuracy: 0.5880\n",
      "Epoch 15/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.2199 - accuracy: 0.9278 - val_loss: 1.8330 - val_accuracy: 0.5760\n",
      "Epoch 16/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.2200 - accuracy: 0.9256 - val_loss: 1.7309 - val_accuracy: 0.5560\n",
      "Epoch 17/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1540 - accuracy: 0.9464 - val_loss: 1.8249 - val_accuracy: 0.5640\n",
      "Epoch 18/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1480 - accuracy: 0.9542 - val_loss: 1.9469 - val_accuracy: 0.5560\n",
      "Epoch 19/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1012 - accuracy: 0.9658 - val_loss: 1.9798 - val_accuracy: 0.5800\n",
      "Epoch 20/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1125 - accuracy: 0.9642 - val_loss: 2.2059 - val_accuracy: 0.5580\n",
      "Test loss: 2.2795145511627197\n",
      "Test accuracy: 0.5649999976158142\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "0.6559\n",
      "Epoch 1/20\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 2.1367 - accuracy: 0.2007 - val_loss: 1.8935 - val_accuracy: 0.3560\n",
      "Epoch 2/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.7702 - accuracy: 0.3487 - val_loss: 1.7322 - val_accuracy: 0.3700\n",
      "Epoch 3/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.5996 - accuracy: 0.4193 - val_loss: 1.6068 - val_accuracy: 0.4080\n",
      "Epoch 4/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.4625 - accuracy: 0.4656 - val_loss: 1.4322 - val_accuracy: 0.4900\n",
      "Epoch 5/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.3535 - accuracy: 0.5140 - val_loss: 1.4776 - val_accuracy: 0.4820\n",
      "Epoch 6/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.2247 - accuracy: 0.5624 - val_loss: 1.3547 - val_accuracy: 0.5220\n",
      "Epoch 7/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 1.1448 - accuracy: 0.5949 - val_loss: 1.3198 - val_accuracy: 0.5340\n",
      "Epoch 8/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.9593 - accuracy: 0.6604 - val_loss: 1.3369 - val_accuracy: 0.5200\n",
      "Epoch 9/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.8194 - accuracy: 0.7122 - val_loss: 1.3779 - val_accuracy: 0.5260\n",
      "Epoch 10/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.7027 - accuracy: 0.7478 - val_loss: 1.3406 - val_accuracy: 0.5740\n",
      "Epoch 11/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.5174 - accuracy: 0.8136 - val_loss: 1.4577 - val_accuracy: 0.5380\n",
      "Epoch 12/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.3971 - accuracy: 0.8647 - val_loss: 1.8229 - val_accuracy: 0.5280\n",
      "Epoch 13/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.3406 - accuracy: 0.8789 - val_loss: 1.7254 - val_accuracy: 0.5520\n",
      "Epoch 14/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.2297 - accuracy: 0.9231 - val_loss: 1.7957 - val_accuracy: 0.5560\n",
      "Epoch 15/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1633 - accuracy: 0.9480 - val_loss: 1.9605 - val_accuracy: 0.5720\n",
      "Epoch 16/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1325 - accuracy: 0.9582 - val_loss: 2.1537 - val_accuracy: 0.5600\n",
      "Epoch 17/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.0990 - accuracy: 0.9678 - val_loss: 2.3145 - val_accuracy: 0.5220\n",
      "Epoch 18/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.1502 - accuracy: 0.9524 - val_loss: 2.2662 - val_accuracy: 0.5360\n",
      "Epoch 19/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.0784 - accuracy: 0.9740 - val_loss: 2.2170 - val_accuracy: 0.5640\n",
      "Epoch 20/20\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.0657 - accuracy: 0.9787 - val_loss: 2.6133 - val_accuracy: 0.5300\n",
      "Test loss: 2.5280849933624268\n",
      "Test accuracy: 0.5860000252723694\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "0.6173\n"
     ]
    }
   ],
   "source": [
    "#データセットを読み込んでテンソルに変換\n",
    "\n",
    "#画像名リスト作成\n",
    "#ここで指定したディレクトリをまとめてnp配列に変換できる。\n",
    "\n",
    "#dcgan_filename = os.listdir('./10000all')\n",
    "dcgan_filename = os.listdir('./20000dcpic')\n",
    "\n",
    "true_filename = os.listdir('./10000harf')\n",
    "\n",
    "test_filename = os.listdir('./500test')\n",
    "\n",
    "#クラスリスト作成\n",
    "#ex:ファイル名に、\"frog\"が入ってたら6、\"truck\"が入っていたら9、のようにラベルを作る。\n",
    "#ラベルは、[0]のように、要素数１のリストとして出力され、ディレクトリ内のｎ個ファイル分だけラベルを作成し、[[6],[1],…,[5]]のようにｎ×１のリストを作る。\n",
    "#最後にリストをnp.array型にして返す。\n",
    "\n",
    "def makeclass(filename):\n",
    "    list1 = []\n",
    "    \n",
    "    for name in filename:\n",
    "        if \"airplane\" in name:\n",
    "            list1.append([0])\n",
    "        elif \"automobile\" in name:\n",
    "            list1.append([1])\n",
    "        elif \"bird\" in name:\n",
    "            list1.append([2])\n",
    "        elif \"cat\" in name:\n",
    "            list1.append([3])\n",
    "        elif \"deer\" in name:\n",
    "            list1.append([4])\n",
    "        elif \"dog\" in name:\n",
    "            list1.append([5])\n",
    "        elif \"frog\" in name:\n",
    "            list1.append([6])\n",
    "        elif \"horse\" in name:\n",
    "            list1.append([7])\n",
    "        elif \"ship\" in name:\n",
    "            list1.append([8])\n",
    "        elif \"truck\" in name:\n",
    "            list1.append([9])\n",
    "        else:\n",
    "            list1.append([10])\n",
    "\n",
    "            \n",
    "    #list型をnp.arrayに変換\n",
    "    classlist = np.array(list1)\n",
    "    return classlist\n",
    "\n",
    "if \".ipynb_checkpoints\" in dcgan_filename:\n",
    "    dcgan_filename.remove(\".ipynb_checkpoints\")     #いらないものを消す。\n",
    "\n",
    "if \".ipynb_checkpoints\" in true_filename:\n",
    "    true_filename.remove(\".ipynb_checkpoints\")\n",
    "\n",
    "if \".ipynb_checkpoints\" in test_filename:\n",
    "    test_filename.remove(\".ipynb_checkpoints\")\n",
    "#print(filename)\n",
    "\n",
    "#統計用リスト\n",
    "scores = []\n",
    "for n in range(len(dcgan_filename)):\n",
    "    scores.append(0)\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    #makeclass関数でラベル作成\n",
    "    dcgan_label = makeclass(dcgan_filename)\n",
    "    true_label = makeclass(true_filename)\n",
    "    test_label = makeclass(test_filename)\n",
    "\n",
    "\n",
    "    #画像ファイル（str型）に相対パスをつなげる。\n",
    "    dcgan_filepass = []\n",
    "    for name in dcgan_filename:\n",
    "        #dcgan_filepass.append(\"./10000all/\"+name)\n",
    "        dcgan_filepass.append(\"./20000dcpic/\"+name)\n",
    "\n",
    "\n",
    "    #print(filepass)\n",
    "\n",
    "    true_filepass = []\n",
    "    for name in true_filename:\n",
    "        true_filepass.append(\"./10000harf/\"+name)\n",
    "\n",
    "\n",
    "    test_filepass = []\n",
    "    for name in test_filename:\n",
    "        test_filepass.append(\"./500test/\"+name)\n",
    "\n",
    "\n",
    "    #dcgan画像\n",
    "    #png画像をndarrayに変換 → 一旦listに直して、imlistに追加\n",
    "    dcgan_imlist = []\n",
    "    for im in dcgan_filepass:\n",
    "        pic = np.array(Image.open(im))\n",
    "        piclist = pic.tolist()\n",
    "        dcgan_imlist.append(piclist)\n",
    "\n",
    "    #imlist（list型）をimarray（ndarray型）に変換\n",
    "    dcgan_imarray = np.array(dcgan_imlist)\n",
    "\n",
    "\n",
    "    #true画像\n",
    "    #png画像をndarrayに変換。→一旦listに直して、imlistに追加\n",
    "    true_imlist = []\n",
    "    for im in true_filepass:\n",
    "        pic = np.array(Image.open(im))\n",
    "        piclist = pic.tolist()\n",
    "        true_imlist.append(piclist)\n",
    "\n",
    "    #imlist（list型）をimarray（ndarray型）に変換\n",
    "    true_imarray = np.array(true_imlist)\n",
    "\n",
    "    #print(np.concatenate([dcgan_imarray, true_imarray]))\n",
    "\n",
    "    #test画像\n",
    "    test_imlist = []\n",
    "\n",
    "    for im in test_filepass:\n",
    "        pic = np.array(Image.open(im))\n",
    "        piclist = pic.tolist()\n",
    "        test_imlist.append(piclist)\n",
    "\n",
    "    #imlist（list型）をimarray（ndarray型）に変換\n",
    "    test_imarray = np.array(test_imlist)\n",
    "\n",
    "    #使用画像選択!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    #Ex_1:CIFAR−１０画像のみ（データ拡張なし）のパターン。\n",
    "    #\n",
    "\n",
    "\n",
    "    xttr = true_imarray[0:5000]\n",
    "    yttr = true_label[0:5000]\n",
    "\n",
    "    xtcon = np.concatenate([true_imarray[0:5000],dcgan_imarray[0:5000]])\n",
    "    ytcon = np.concatenate([true_label[0:5000], dcgan_label[0:5000]])\n",
    "\n",
    "    (x_train, y_train) = (xttr, yttr)\n",
    "    (x_test, y_test) = (test_imarray, test_label)\n",
    "\n",
    "\n",
    "    #画像を0-1の範囲で正規化\n",
    "    x_train=x_train.astype('float32')/255.0\n",
    "    x_test=x_test.astype('float32')/255.0\n",
    "\n",
    "    #正解ラベルをOne-Hot表現に変換\n",
    "    y_train=np_utils.to_categorical(y_train,10)\n",
    "    y_test=np_utils.to_categorical(y_test,10)\n",
    "\n",
    "    \n",
    "\n",
    "    #モデルを構築\n",
    "    model=Sequential()\n",
    "\n",
    "    model.add(Conv2D(64,(3,3),padding='same',input_shape=(32,32,3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64,(3,3),padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(128,(3,3),padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(128,(3,3),padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "    history=model.fit(x_train,y_train,batch_size=128,epochs=20,verbose=1,validation_split=0.1)\n",
    "\n",
    "    #モデルと重みを保存\n",
    "    json_string=model.to_json()\n",
    "    open('cifar10_cnn.json',\"w\").write(json_string)\n",
    "    model.save_weights('cifar10_cnn.h5')\n",
    "\n",
    "    #モデルの表示\n",
    "    #model.summary()\n",
    "\n",
    "    #評価\n",
    "    score=model.evaluate(x_test,y_test,verbose=0)\n",
    "    print('Test loss:',score[0])\n",
    "    print('Test accuracy:',score[1])\n",
    "\n",
    "\n",
    "    #使えるor使えないの２択分類器（学習後のモデルを用いて、元のクラス＝predictのクラス→「使える」、もとのクラス≠predictのクラス→「使えない」に振り分ける。）\n",
    "    #ここでの出力は、ｘ_testの通し番号をリストにまとめたものなので、この通し番号を用いてx_testの何番目の画像かを調べることが可能。ただし、X_testの数値（通し番号）とファイル名の番号は違うことに注意。\n",
    "    #ex：「generated_truck392.png」の通し番号は1032、等\n",
    "    #この通し番号は「画像ファイル名」、「スコア」、「x_testの何番目か」という情報を紐付けしている。\n",
    "    acc_count = []\n",
    "    usable = []\n",
    "    unusable = []\n",
    "\n",
    "    y_test=np_utils.to_categorical(dcgan_label,10)\n",
    "\n",
    "    y_predict = model.predict(dcgan_imarray, batch_size=32)     #学習済みCNNを用いてクラスを予測。\n",
    "    predict_classes = np.argmax(y_predict,1)                    #one-hot表現になっているものをラベルと同じ形式（[1]など）に直す。\n",
    "    true_classes = np.argmax(y_test,1)                          #もともとのラベルも↑と同じ形式に直す。\n",
    "\n",
    "    for i in range(len(predict_classes)):\n",
    "        if predict_classes[i] == true_classes[i]:\n",
    "            acc_count.append(1)\n",
    "            usable.append(i)\n",
    "        else:\n",
    "            acc_count.append(0)\n",
    "            unusable.append(i)\n",
    "\n",
    "\n",
    "    acc = np.average(acc_count)\n",
    "    print(acc)\n",
    "    #print(usable)\n",
    "    #print(unusable)\n",
    "    \n",
    "    for m in range(len(usable)):\n",
    "        scores[m] += 1\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "719bca41-2ea6-4aab-adbc-a377cc0c5334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "#ファイルの選別（usable/unusable）\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "target_dirT = './TP'\n",
    "shutil.rmtree(target_dirT)\n",
    "os.mkdir(target_dirT)\n",
    "\n",
    "target_dirF = './FP'\n",
    "shutil.rmtree(target_dirF)\n",
    "os.mkdir(target_dirF)\n",
    "\n",
    "TPlist = []\n",
    "FPlist = []\n",
    "\n",
    "for l in range(len(dcgan_filename)):\n",
    "    if scores[l] > 8:\n",
    "        TPlist.append(l)\n",
    "        \n",
    "    elif scores[l] < 2:\n",
    "        FPlist.append(l)\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "\n",
    "for i in range(len(TPlist)):\n",
    "    file = \"./20000dcpic/\"+str(dcgan_filename[TPlist[i]])\n",
    "    target = \"TP/{}\".format(str(dcgan_filename[TPlist[i]]))\n",
    "    shutil.copyfile(file, target)\n",
    "\n",
    "\n",
    "for j in range(len(FPlist)):\n",
    "    file = \"./20000dcpic/\"+str(dcgan_filename[FPlist[j]])\n",
    "    target = \"FP/{}\".format(str(dcgan_filename[FPlist[j]]))\n",
    "    shutil.copyfile(file, target)\n",
    "\n",
    "\n",
    "print(\"finish\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da55a695-d689-4d10-94ff-f8b8f82e7e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TP>\n",
      "0     1176\n",
      "1     1188\n",
      "2     1187\n",
      "3     1136\n",
      "4     1185\n",
      "5     1243\n",
      "6     1153\n",
      "7     1196\n",
      "8     1224\n",
      "9     1203\n",
      "10       0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<FP>\n",
      "0     694\n",
      "1     688\n",
      "2     696\n",
      "3     742\n",
      "4     667\n",
      "5     653\n",
      "6     719\n",
      "7     686\n",
      "8     662\n",
      "9     675\n",
      "10      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fileTP = os.listdir('./TP')\n",
    "fileFP = os.listdir('./FP')\n",
    "\n",
    "\n",
    "def classcount(filename):\n",
    "    count = [0,0,0,0,0,0,0,0,0,0,0]\n",
    "    \n",
    "    for name in filename:\n",
    "        if \"airplane\" in name:\n",
    "            count[0] += 1\n",
    "        elif \"automobile\" in name:\n",
    "            count[1] += 1\n",
    "        elif \"bird\" in name:\n",
    "            count[2] += 1\n",
    "        elif \"cat\" in name:\n",
    "            count[3] += 1\n",
    "        elif \"deer\" in name:\n",
    "            count[4] += 1\n",
    "        elif \"dog\" in name:\n",
    "            count[5] += 1\n",
    "        elif \"frog\" in name:\n",
    "            count[6] += 1\n",
    "        elif \"horse\" in name:\n",
    "            count[7] += 1\n",
    "        elif \"ship\" in name:\n",
    "            count[8] += 1\n",
    "        elif \"truck\" in name:\n",
    "            count[9] += 1\n",
    "        else:\n",
    "            count[10] += 1\n",
    "            print(name)\n",
    "            \n",
    "    s1 = pd.Series(count)\n",
    "    return s1\n",
    "\n",
    "print(\"<TP>\")\n",
    "print(classcount(fileTP))\n",
    "print(\"\\n\\n\\n\\n<FP>\")\n",
    "print(classcount(fileFP))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1cfba5f-bb69-4a7f-bba4-b2fab52e7269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        10\n",
      "1        10\n",
      "2        10\n",
      "3        10\n",
      "4        10\n",
      "         ..\n",
      "19995     0\n",
      "19996     0\n",
      "19997     0\n",
      "19998     0\n",
      "19999     0\n",
      "Length: 20000, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([6.5140e+03, 3.6800e+02, 2.8100e+02, 2.4500e+02, 2.8000e+01,\n",
       "        8.0000e+00, 2.1000e+02, 1.6300e+02, 2.9200e+02, 1.1891e+04]),\n",
       " array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.Series(scores)\n",
    "print(df)\n",
    "\n",
    "plt.hist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59fb92cb-2085-4a0f-9649-08d055ddae33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      6514\n",
      "1       368\n",
      "2       281\n",
      "3       245\n",
      "4        28\n",
      "5         8\n",
      "6       210\n",
      "7       163\n",
      "8       292\n",
      "9       750\n",
      "10    11141\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def scorecount(filename):\n",
    "    count = [0,0,0,0,0,0,0,0,0,0,0]\n",
    "    \n",
    "    for name in filename:\n",
    "        if name==0:\n",
    "            count[0] += 1\n",
    "        elif name==1:\n",
    "            count[1] += 1\n",
    "        elif name==2:\n",
    "            count[2] += 1\n",
    "        elif name==3:\n",
    "            count[3] += 1\n",
    "        elif name==4:\n",
    "            count[4] += 1\n",
    "        elif name==5:\n",
    "            count[5] += 1\n",
    "        elif name==6:\n",
    "            count[6] += 1\n",
    "        elif name==7:\n",
    "            count[7] += 1\n",
    "        elif name==8:\n",
    "            count[8] += 1\n",
    "        elif name==9:\n",
    "            count[9] += 1\n",
    "        else:\n",
    "            count[10] += 1\n",
    "            #print(name)\n",
    "            \n",
    "    s1 = pd.Series(count)\n",
    "    return s1\n",
    "\n",
    "print(scorecount(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e48db0d-e886-4d0e-b4e6-4dda5a2510b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 610,\n",
       " 611,\n",
       " 612,\n",
       " 613,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 621,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 635,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 641,\n",
       " 642,\n",
       " 643,\n",
       " 644,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 653,\n",
       " 654,\n",
       " 655,\n",
       " 656,\n",
       " 657,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 663,\n",
       " 664,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 668,\n",
       " 669,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 676,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 684,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 688,\n",
       " 689,\n",
       " 690,\n",
       " 691,\n",
       " 692,\n",
       " 693,\n",
       " 694,\n",
       " 695,\n",
       " 696,\n",
       " 697,\n",
       " 698,\n",
       " 699,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 703,\n",
       " 704,\n",
       " 705,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 709,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 713,\n",
       " 714,\n",
       " 715,\n",
       " 716,\n",
       " 717,\n",
       " 718,\n",
       " 719,\n",
       " 720,\n",
       " 721,\n",
       " 722,\n",
       " 723,\n",
       " 724,\n",
       " 725,\n",
       " 726,\n",
       " 727,\n",
       " 728,\n",
       " 729,\n",
       " 730,\n",
       " 731,\n",
       " 732,\n",
       " 733,\n",
       " 734,\n",
       " 735,\n",
       " 736,\n",
       " 737,\n",
       " 738,\n",
       " 739,\n",
       " 740,\n",
       " 741,\n",
       " 742,\n",
       " 743,\n",
       " 744,\n",
       " 745,\n",
       " 746,\n",
       " 747,\n",
       " 748,\n",
       " 749,\n",
       " 750,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 755,\n",
       " 756,\n",
       " 757,\n",
       " 758,\n",
       " 759,\n",
       " 760,\n",
       " 761,\n",
       " 762,\n",
       " 763,\n",
       " 764,\n",
       " 765,\n",
       " 766,\n",
       " 767,\n",
       " 768,\n",
       " 769,\n",
       " 770,\n",
       " 771,\n",
       " 772,\n",
       " 773,\n",
       " 774,\n",
       " 775,\n",
       " 776,\n",
       " 777,\n",
       " 778,\n",
       " 779,\n",
       " 780,\n",
       " 781,\n",
       " 782,\n",
       " 783,\n",
       " 784,\n",
       " 785,\n",
       " 786,\n",
       " 787,\n",
       " 788,\n",
       " 789,\n",
       " 790,\n",
       " 791,\n",
       " 792,\n",
       " 793,\n",
       " 794,\n",
       " 795,\n",
       " 796,\n",
       " 797,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 801,\n",
       " 802,\n",
       " 803,\n",
       " 804,\n",
       " 805,\n",
       " 806,\n",
       " 807,\n",
       " 808,\n",
       " 809,\n",
       " 810,\n",
       " 811,\n",
       " 812,\n",
       " 813,\n",
       " 814,\n",
       " 815,\n",
       " 816,\n",
       " 817,\n",
       " 818,\n",
       " 819,\n",
       " 820,\n",
       " 821,\n",
       " 822,\n",
       " 823,\n",
       " 824,\n",
       " 825,\n",
       " 826,\n",
       " 827,\n",
       " 828,\n",
       " 829,\n",
       " 830,\n",
       " 831,\n",
       " 832,\n",
       " 833,\n",
       " 834,\n",
       " 835,\n",
       " 836,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 840,\n",
       " 841,\n",
       " 842,\n",
       " 843,\n",
       " 844,\n",
       " 845,\n",
       " 846,\n",
       " 847,\n",
       " 848,\n",
       " 849,\n",
       " 850,\n",
       " 851,\n",
       " 852,\n",
       " 853,\n",
       " 854,\n",
       " 855,\n",
       " 856,\n",
       " 857,\n",
       " 858,\n",
       " 859,\n",
       " 860,\n",
       " 861,\n",
       " 862,\n",
       " 863,\n",
       " 864,\n",
       " 865,\n",
       " 866,\n",
       " 867,\n",
       " 868,\n",
       " 869,\n",
       " 870,\n",
       " 871,\n",
       " 872,\n",
       " 873,\n",
       " 874,\n",
       " 875,\n",
       " 876,\n",
       " 877,\n",
       " 878,\n",
       " 879,\n",
       " 880,\n",
       " 881,\n",
       " 882,\n",
       " 883,\n",
       " 884,\n",
       " 885,\n",
       " 886,\n",
       " 887,\n",
       " 888,\n",
       " 889,\n",
       " 890,\n",
       " 891,\n",
       " 892,\n",
       " 893,\n",
       " 894,\n",
       " 895,\n",
       " 896,\n",
       " 897,\n",
       " 898,\n",
       " 899,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 903,\n",
       " 904,\n",
       " 905,\n",
       " 906,\n",
       " 907,\n",
       " 908,\n",
       " 909,\n",
       " 910,\n",
       " 911,\n",
       " 912,\n",
       " 913,\n",
       " 914,\n",
       " 915,\n",
       " 916,\n",
       " 917,\n",
       " 918,\n",
       " 919,\n",
       " 920,\n",
       " 921,\n",
       " 922,\n",
       " 923,\n",
       " 924,\n",
       " 925,\n",
       " 926,\n",
       " 927,\n",
       " 928,\n",
       " 929,\n",
       " 930,\n",
       " 931,\n",
       " 932,\n",
       " 933,\n",
       " 934,\n",
       " 935,\n",
       " 936,\n",
       " 937,\n",
       " 938,\n",
       " 939,\n",
       " 940,\n",
       " 941,\n",
       " 942,\n",
       " 943,\n",
       " 944,\n",
       " 945,\n",
       " 946,\n",
       " 947,\n",
       " 948,\n",
       " 949,\n",
       " 950,\n",
       " 951,\n",
       " 952,\n",
       " 953,\n",
       " 954,\n",
       " 955,\n",
       " 956,\n",
       " 957,\n",
       " 958,\n",
       " 959,\n",
       " 960,\n",
       " 961,\n",
       " 962,\n",
       " 963,\n",
       " 964,\n",
       " 965,\n",
       " 966,\n",
       " 967,\n",
       " 968,\n",
       " 969,\n",
       " 970,\n",
       " 971,\n",
       " 972,\n",
       " 973,\n",
       " 974,\n",
       " 975,\n",
       " 976,\n",
       " 977,\n",
       " 978,\n",
       " 979,\n",
       " 980,\n",
       " 981,\n",
       " 982,\n",
       " 983,\n",
       " 984,\n",
       " 985,\n",
       " 986,\n",
       " 987,\n",
       " 988,\n",
       " 989,\n",
       " 990,\n",
       " 991,\n",
       " 992,\n",
       " 993,\n",
       " 994,\n",
       " 995,\n",
       " 996,\n",
       " 997,\n",
       " 998,\n",
       " 999,\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TPlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62010b4-6980-4d4f-948c-1794fe6fab87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
